from fastapi import FastAPI, File, UploadFile, Form
from typing import Optional, List, Dict, Any
from PIL import Image
import io, json, numpy as np, onnxruntime as ort
import torchvision.transforms as T
import re, time, uuid
import sys
import os
import warnings

# Suppress transformers warnings
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)

# Import NSFW detector t·ª´ c√πng th∆∞ m·ª•c
from nsfw_detector import NSFWDetector

# Import Title Generator
from title_generator import generate_titles

# AI Caption model - Florence-2-large for vivid descriptions
from transformers import AutoProcessor, AutoModelForCausalLM
import torch

app = FastAPI(title="AI Title Service")

# Load classifier (ONNX) + meta
META = json.load(open("models/meta.json", "r", encoding="utf-8"))
LABELS = META["labels"]; THRESH = float(META.get("threshold", 0.5))
ORT_SESS = ort.InferenceSession("models/cls.onnx", providers=["CPUExecutionProvider"])
TF = T.Compose([T.Resize((224,224)), T.ToTensor()])

# Load NSFW detector
NSFW_DETECTOR = NSFWDetector("LukeJacob2023/nsfw-image-detector")

# AI Caption - S·ª≠ d·ª•ng BLIP-base (nh·∫π h∆°n, ph√π h·ª£p v·ªõi RAM th·∫•p)
try:
    from transformers import BlipProcessor, BlipForConditionalGeneration
    
    # BLIP-base ch·ªâ ~1GB, nh·∫π h∆°n BLIP-2 r·∫•t nhi·ªÅu
    CAP_PROCESSOR = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
    CAP_MODEL = BlipForConditionalGeneration.from_pretrained(
        "Salesforce/blip-image-captioning-base"
    )
    CAPTION_ENABLED = True
except Exception as e:
    print(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ t·∫£i BLIP: {e}")
    CAP_MODEL = None
    CAP_PROCESSOR = None
    CAPTION_ENABLED = False

# Translator EN->VI - S·ª≠ d·ª•ng MarianMT (nh·∫π, nhanh)
try:
    from transformers import MarianMTModel, MarianTokenizer
    
    # Helsinki-NLP MarianMT cho English -> Vietnamese
    TRANSLATOR_MODEL_NAME = "Helsinki-NLP/opus-mt-en-vi"
    TRANSLATOR_TOKENIZER = MarianTokenizer.from_pretrained(TRANSLATOR_MODEL_NAME)
    TRANSLATOR_MODEL = MarianMTModel.from_pretrained(TRANSLATOR_MODEL_NAME)
    TRANSLATOR_ENABLED = True
except Exception as e:
    print(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ t·∫£i MarianMT: {e}")
    TRANSLATOR_MODEL = None
    TRANSLATOR_TOKENIZER = None
    TRANSLATOR_ENABLED = False

# Th√¥ng b√°o kh·ªüi ƒë·ªông xong
print("‚úÖ All models ready")

# T·ª´ ƒëi·ªÉn d·ªãch EN->VI ƒë·ªÉ t·∫°o caption ti·∫øng Vi·ªát t·ª± nhi√™n
KW_EN_VI = {
  # Thi√™n nhi√™n
  "mountain": "n√∫i", "mountains": "nh·ªØng ng·ªçn n√∫i", "hill": "ƒë·ªìi", "hills": "nh·ªØng ng·ªçn ƒë·ªìi",
  "river": "d√≤ng s√¥ng", "lake": "h·ªì", "sea": "bi·ªÉn", "ocean": "ƒë·∫°i d∆∞∆°ng",
  "beach": "b√£i bi·ªÉn", "forest": "khu r·ª´ng", "tree": "c√¢y", "trees": "nh·ªØng c√¢y",
  "field": "c√°nh ƒë·ªìng", "fields": "nh·ªØng c√°nh ƒë·ªìng", "rice": "l√∫a", "rice field": "c√°nh ƒë·ªìng l√∫a",
  "sun": "m·∫∑t tr·ªùi", "sunshine": "√°nh n·∫Øng", "shining": "chi·∫øu s√°ng", "bright": "s√°ng",
  "sky": "b·∫ßu tr·ªùi", "cloud": "m√¢y", "clouds": "nh·ªØng ƒë√°m m√¢y",
  
  # Ki·∫øn tr√∫c & Di s·∫£n
  "temple": "ƒë·ªÅn ch√πa", "church": "nh√† th·ªù", "cathedral": "nh√† th·ªù l·ªõn",
  "bridge": "c√¢y c·∫ßu", "building": "t√≤a nh√†", "buildings": "nh·ªØng t√≤a nh√†",
  "house": "ng√¥i nh√†", "houses": "nh·ªØng ng√¥i nh√†", "village": "l√†ng", "town": "th·ªã tr·∫•n",
  "city": "th√†nh ph·ªë", "ancient": "c·ªï", "old": "c≈©", "traditional": "truy·ªÅn th·ªëng",
  "monument": "di t√≠ch", "architecture": "ki·∫øn tr√∫c", "pagoda": "ch√πa",
  
  # VƒÉn h√≥a & Con ng∆∞·ªùi
  "festival": "l·ªÖ h·ªôi", "market": "ch·ª£", "people": "ng∆∞·ªùi d√¢n", "person": "ng∆∞·ªùi",
  "woman": "ph·ª• n·ªØ", "man": "ƒë√†n √¥ng", "child": "tr·∫ª em", "children": "nh·ªØng ƒë·ª©a tr·∫ª",
  "family": "gia ƒë√¨nh", "group": "nh√≥m", "crowd": "ƒë√°m ƒë√¥ng",
  
  # M√†u s·∫Øc & Tr·∫°ng th√°i
  "green": "xanh l√°", "blue": "xanh d∆∞∆°ng", "red": "ƒë·ªè", "yellow": "v√†ng",
  "white": "tr·∫Øng", "black": "ƒëen", "beautiful": "ƒë·∫πp", "peaceful": "y√™n b√¨nh",
  "quiet": "y√™n tƒ©nh", "busy": "nh·ªôn nh·ªãp", "crowded": "ƒë√¥ng ƒë√∫c",
  
  # Gi·ªõi t·ª´ & ƒê·ªông t·ª´
  "over": "tr√™n", "under": "d∆∞·ªõi", "beside": "b√™n c·∫°nh", "near": "g·∫ßn",
  "in": "trong", "on": "tr√™n", "at": "t·∫°i", "with": "v·ªõi",
  "walking": "ƒëang ƒëi b·ªô", "standing": "ƒëang ƒë·ª©ng", "sitting": "ƒëang ng·ªìi",
  "playing": "ƒëang ch∆°i", "working": "ƒëang l√†m vi·ªác"
}

THEME_PREFIX = {
  "Heritage & Architecture": "Di s·∫£n & Ki·∫øn tr√∫c",
  "Culture & Art": "VƒÉn ho√° & Ngh·ªá thu·∫≠t",
  "Nature & Landscape": "Thi√™n nhi√™n & C·∫£nh quan",
  "People & Events": "Con ng∆∞·ªùi & S·ª± ki·ªán"
}

def softmax(x):
    e = np.exp(x - np.max(x))
    return e / e.sum()

def sigmoid(x): return 1/(1+np.exp(-x))

def translate_caption_to_vi(caption_en: str) -> str:
    """D·ªãch nhanh caption ti·∫øng Anh sang ti·∫øng Vi·ªát b·∫±ng t·ª´ ƒëi·ªÉn"""
    if not caption_en:
        return ""
    
    # Lowercase ƒë·ªÉ d·ªÖ match
    caption_lower = caption_en.lower()
    caption_vi = caption_en
    
    # Thay th·∫ø c√°c t·ª´ kh√≥a c√≥ trong t·ª´ ƒëi·ªÉn
    for en_word, vi_word in KW_EN_VI.items():
        # Thay th·∫ø whole word
        import re
        pattern = r'\b' + re.escape(en_word) + r'\b'
        caption_vi = re.sub(pattern, vi_word, caption_vi, flags=re.IGNORECASE)
    
    # C√°c c·ª•m t·ª´ th∆∞·ªùng g·∫∑p
    replacements = {
        "a photo of": "·∫¢nh ch·ª•p",
        "an image of": "H√¨nh ·∫£nh",
        "shows": "cho th·∫•y",
        "showing": "ƒëang hi·ªÉn th·ªã",
        "with": "v·ªõi",
        "and": "v√†",
        "in the": "trong",
        "on the": "tr√™n",
        "at the": "t·∫°i",
        "near the": "g·∫ßn"
    }
    
    for en_phrase, vi_phrase in replacements.items():
        caption_vi = re.sub(r'\b' + re.escape(en_phrase) + r'\b', vi_phrase, caption_vi, flags=re.IGNORECASE)
    
    # L√†m s·∫°ch v√† vi·∫øt hoa ch·ªØ c√°i ƒë·∫ßu
    caption_vi = caption_vi.strip()
    if caption_vi:
        caption_vi = caption_vi[0].upper() + caption_vi[1:]
    
    return caption_vi

def translate_en_to_vi(text_en: str) -> str:
    """D·ªãch ti·∫øng Anh sang ti·∫øng Vi·ªát b·∫±ng MarianMT"""
    if not TRANSLATOR_ENABLED or TRANSLATOR_MODEL is None or TRANSLATOR_TOKENIZER is None:
        return text_en  # Gi·ªØ nguy√™n ti·∫øng Anh n·∫øu kh√¥ng c√≥ translator
    
    try:
        # Tokenize v√† d·ªãch
        inputs = TRANSLATOR_TOKENIZER(text_en, return_tensors="pt", padding=True)
        
        with torch.no_grad():
            translated = TRANSLATOR_MODEL.generate(**inputs, max_new_tokens=100)
        
        # Decode k·∫øt qu·∫£
        text_vi = TRANSLATOR_TOKENIZER.decode(translated[0], skip_special_tokens=True).strip()
        
        # Vi·∫øt hoa ch·ªØ c√°i ƒë·∫ßu
        if text_vi:
            text_vi = text_vi[0].upper() + text_vi[1:] if len(text_vi) > 1 else text_vi.upper()
        
        return text_vi if text_vi else text_en
        
    except Exception as e:
        print(f"‚ö†Ô∏è  L·ªói khi d·ªãch: {e}")
        return text_en

def generate_caption_with_blip2(pil: Image.Image) -> tuple:
    """T·∫°o caption b·∫±ng BLIP AI model v√† d·ªãch sang ti·∫øng Vi·ªát
    
    Returns:
        tuple: (caption_en, caption_vi)
    """
    if not CAPTION_ENABLED or CAP_MODEL is None or CAP_PROCESSOR is None:
        return None, None
    
    try:
        # Process image
        inputs = CAP_PROCESSOR(images=pil, return_tensors="pt")
        
        # Move to same device as model if GPU available
        if torch.cuda.is_available():
            inputs = {k: v.to(CAP_MODEL.device) for k, v in inputs.items()}
        
        # Generate caption
        with torch.no_grad():
            generated_ids = CAP_MODEL.generate(
                **inputs,
                max_new_tokens=50,  # Caption ng·∫Øn g·ªçn
                num_beams=3,
                early_stopping=True
            )
        
        # Decode caption ti·∫øng Anh
        caption_en = CAP_PROCESSOR.decode(generated_ids[0], skip_special_tokens=True).strip()
        
        # Vi·∫øt hoa ch·ªØ c√°i ƒë·∫ßu n·∫øu ch∆∞a c√≥
        if caption_en and not caption_en[0].isupper():
            caption_en = caption_en[0].upper() + caption_en[1:]
        
        # D·ªãch sang ti·∫øng Vi·ªát
        caption_vi = translate_en_to_vi(caption_en)
        
        return caption_en, caption_vi
        
    except Exception as e:
        print(f"‚ö†Ô∏è  L·ªói khi t·∫°o caption v·ªõi BLIP: {e}")
        return None, None

def detect_people_in_image(pil: Image.Image) -> bool:
    """Ph√°t hi·ªán s·ª± hi·ªán di·ªán c·ªßa con ng∆∞·ªùi trong ·∫£nh v·ªõi ƒë·ªô ch√≠nh x√°c cao h∆°n."""
    try:
        # Resize ·∫£nh ƒë·ªÉ ph√¢n t√≠ch nhanh
        small_img = pil.resize((128, 128))
        
        # Chuy·ªÉn sang RGB ƒë·ªÉ ph√¢n t√≠ch m√†u da ch√≠nh x√°c h∆°n
        rgb_img = small_img.convert('RGB')
        pixels = list(rgb_img.getdata())
        
        # Ph√¢n t√≠ch m√†u da ng∆∞·ªùi v·ªõi nhi·ªÅu ƒëi·ªÅu ki·ªán
        skin_pixels = 0
        total_pixels = len(pixels)
        
        for r, g, b in pixels:
            # ƒêi·ªÅu ki·ªán 1: Da ng∆∞·ªùi b√¨nh th∆∞·ªùng
            normal_skin = (r > 95 and g > 40 and b > 20 and 
                          r > g and g > b and 
                          abs(r - g) > 15 and 
                          r > 60 and g > 30 and b > 15)
            
            # ƒêi·ªÅu ki·ªán 2: Da ng∆∞·ªùi b·ªã bi·∫øn d·∫°ng m√†u (xanh l·ª•c/v√†ng)
            distorted_skin = (g > 80 and r > 60 and b < 100 and 
                             abs(g - r) < 50 and 
                             g > b and r > b)
            
            # ƒêi·ªÅu ki·ªán 3: Da ng∆∞·ªùi t·ªëi m√†u
            dark_skin = (r > 40 and g > 30 and b > 20 and 
                        r > g and g > b and 
                        r < 120 and g < 100 and b < 80)
            
            if normal_skin or distorted_skin or dark_skin:
                skin_pixels += 1
        
        skin_ratio = skin_pixels / total_pixels
        
        # Gi·∫£m ng∆∞·ª°ng cho ·∫£nh ch·∫•t l∆∞·ª£ng th·∫•p (t·ª´ 25% xu·ªëng 15%)
        # V√† gi·∫£m s·ªë pixel t·ªëi thi·ªÉu (t·ª´ 1000 xu·ªëng 500)
        return skin_ratio > 0.15 and skin_pixels > 500
        
    except:
        return False

def generate_caption_vi(pil: Image.Image, predicted_labels: List[str] = None, confidence_scores: List[float] = None) -> str:
    """T·∫°o caption ti·∫øng Vi·ªát chuy√™n nghi·ªáp v·ªõi 3 phi√™n b·∫£n kh√°c nhau."""
    import random
    
    width, height = pil.size
    aspect_ratio = width / height
    
    # Ph√¢n t√≠ch m√†u s·∫Øc v√† √°nh s√°ng
    try:
        small_img = pil.resize((64, 64))
        colors = small_img.getcolors(maxcolors=256*256*256)
        
        color_analysis = {
            "dominant": "ƒëa d·∫°ng",
            "brightness": "trung b√¨nh",
            "mood": "trung t√≠nh"
        }
        
        if colors:
            dominant_color = max(colors, key=lambda x: x[0])[1]
            r, g, b = dominant_color
            
            # X√°c ƒë·ªãnh m√†u ch·ªß ƒë·∫°o v√† t√¢m tr·∫°ng
            if r > 200 and g < 100 and b < 100:
                color_analysis["dominant"] = "ƒë·ªè"
                color_analysis["mood"] = "·∫•m √°p"
            elif g > 200 and r < 100 and b < 100:
                color_analysis["dominant"] = "xanh l√°"
                color_analysis["mood"] = "t∆∞∆°i m√°t"
            elif b > 200 and r < 100 and g < 100:
                color_analysis["dominant"] = "xanh d∆∞∆°ng"
                color_analysis["mood"] = "thanh b√¨nh"
            elif r > 200 and g > 200 and b < 100:
                color_analysis["dominant"] = "v√†ng"
                color_analysis["mood"] = "r·ª±c r·ª°"
            elif r > 150 and g > 150 and b > 150:
                color_analysis["dominant"] = "s√°ng"
                color_analysis["brightness"] = "r·ª±c r·ª°"
            elif r < 100 and g < 100 and b < 100:
                color_analysis["dominant"] = "t·ªëi"
                color_analysis["brightness"] = "d·ªãu nh·∫π"
    except:
        pass
    
    # Ph√°t hi·ªán con ng∆∞·ªùi trong ·∫£nh (v·ªõi ƒë·ªô ch√≠nh x√°c cao h∆°n)
    has_people = detect_people_in_image(pil)
    
    # Ki·ªÉm tra confidence scores
    low_confidence = False
    if confidence_scores and len(confidence_scores) > 0:
        max_confidence = max(confidence_scores)
        low_confidence = max_confidence < 0.6  # Gi·∫£m ng∆∞·ª°ng xu·ªëng 60%
    
    # Ch·ªâ override khi th·ª±c s·ª± ch·∫Øc ch·∫Øn c√≥ ng∆∞·ªùi V√Ä model kh√¥ng nh·∫≠n ra
    if (has_people and 
        predicted_labels and 
        "People & Events" not in predicted_labels and 
        not low_confidence):
        # Ch·ªâ override khi confidence c·ªßa People & Events th·∫•p h∆°n 0.3
        people_scores = [s for n, s in zip(predicted_labels, confidence_scores) if n == "People & Events"]
        if not people_scores or people_scores[0] < 0.3:
            predicted_labels = ["People & Events"]
    
    # X√°c ƒë·ªãnh lo·∫°i ·∫£nh
    if aspect_ratio > 1.5:
        image_type = "wide"
    elif aspect_ratio < 0.7:
        image_type = "tall"
    else:
        image_type = "square"
    
    # Template captions chuy√™n nghi·ªáp theo 3 phong c√°ch
    # Template caption ng·∫Øn g·ªçn (40-60 k√Ω t·ª±)
    professional_templates = {
        "Nature & Landscape": [
            "Thi√™n nhi√™n h√πng vƒ©",
            "C·∫£nh quan thi√™n nhi√™n ƒë·∫πp",
            "Khung c·∫£nh thi√™n nhi√™n",
            "Thi√™n nhi√™n hoang s∆°"
        ],
        "Heritage & Architecture": [
            "Ki·∫øn tr√∫c c·ªï k√≠nh",
            "Di s·∫£n vƒÉn h√≥a",
            "C√¥ng tr√¨nh l·ªãch s·ª≠",
            "Ki·∫øn tr√∫c truy·ªÅn th·ªëng"
        ],
        "Culture & Art": [
            "VƒÉn h√≥a s√¥i ƒë·ªông",
            "Kh√¥ng gian ngh·ªá thu·∫≠t",
            "VƒÉn h√≥a truy·ªÅn th·ªëng",
            "Ngh·ªá thu·∫≠t ƒë·∫∑c s·∫Øc"
        ],
        "People & Events": [
            "Kho·∫£nh kh·∫Øc ƒë√°ng nh·ªõ",
            "C·ªông ƒë·ªìng g·∫Øn k·∫øt",
            "S·ª± ki·ªán √Ω nghƒ©a",
            "Con ng∆∞·ªùi v√† ho·∫°t ƒë·ªông"
        ]
    }
    
    # Ch·ªçn template d·ª±a tr√™n predicted labels
    if predicted_labels:
        for label in predicted_labels:
            if label in professional_templates:
                base_caption = random.choice(professional_templates[label])
                break
        else:
            # Fallback
            base_caption = random.choice([
                "Khung c·∫£nh ƒë·∫πp",
                "C·∫£nh quan ·∫•n t∆∞·ª£ng",
                "Kh√¥ng gian tuy·ªát v·ªùi"
            ])
    else:
        base_caption = random.choice([
            "Khung c·∫£nh ƒë·∫πp",
            "C·∫£nh quan ·∫•n t∆∞·ª£ng",
            "Kh√¥ng gian tuy·ªát v·ªùi"
        ])
    
    # Caption ng·∫Øn g·ªçn, kh√¥ng th√™m m√†u s·∫Øc ƒë·ªÉ tr√°nh r∆∞·ªùm r√†
    
    return base_caption

def fuse_titles(top_labels: List[str], cap_vi: str) -> List[str]:
    """Gh√©p nh√£n + caption th√†nh 2-3 ti√™u ƒë·ªÅ g·ª£i √Ω."""
    # ti·ªÅn t·ªë theo nh√£n (t·ªëi ƒëa 1-2 nh√£n n·ªïi tr·ªôi)
    prefixes = [THEME_PREFIX.get(x, x) for x in top_labels[:2]]
    pfx = " ‚Ä¢ ".join(prefixes) if prefixes else ""
    base = cap_vi
    base = re.sub(r"(?i)\b(h√¨nh ·∫£nh|·∫£nh|a photo of)\b", "", base).strip()
    base = re.sub(r"\s+", " ", base)

    titles = []
    if pfx and base:
        titles.append(f"{pfx}: {base}")
    if base:
        titles.append(base)
        short = " ".join(base.split()[:8])
        if short not in titles: titles.append(short)

    # kh·ª≠ tr√πng l·∫∑p/ k√Ω t·ª± th·ª´a
    seen=set(); out=[]
    for t in titles:
        t = t.strip(" -‚Ä¢:‚Äì")
        if t and t.lower() not in seen:
            out.append(t); seen.add(t.lower())
    return out[:3] if out else ["Kho·∫£nh kh·∫Øc ƒë√°ng nh·ªõ"]

@app.post("/analyze")
async def analyze(
    file: UploadFile = File(...),
    override_labels_json: Optional[str] = Form(None),  # n·∫øu mu·ªën g·ª≠i danh s√°ch label ri√™ng
    generate_title: Optional[bool] = Form(True)  # c√≥ t·∫°o title hay kh√¥ng (m·∫∑c ƒë·ªãnh: c√≥)
) -> Dict[str, Any]:
    image_id = str(uuid.uuid4())
    raw = await file.read()
    pil = Image.open(io.BytesIO(raw)).convert("RGB")

    # B∆∞·ªõc 1: Ki·ªÉm tra NSFW tr∆∞·ªõc
    # print(f"üîç [Image {image_id}] ƒêang ki·ªÉm tra NSFW...")  # B·ªè log chi ti·∫øt
    nsfw_result = NSFW_DETECTOR.predict_single_image(pil)
    
    # X·ª≠ l√Ω theo m·ª©c ƒë·ªô vi ph·∫°m
    if nsfw_result['level'] == 'severe':
        # Ch·∫∑n ho√†n to√†n - kh√¥ng ch·∫°y classifier
        print(f"üö´ [Image {image_id}] ·∫¢nh b·ªã ch·∫∑n - m·ª©c ƒë·ªô nghi√™m tr·ªçng")
        return {
            "imageId": image_id,
            "nsfw_check": {
                "predicted_label": nsfw_result['predicted_label'],
                "is_violation": nsfw_result['is_violation'],
                "level": nsfw_result['level'],
                "message": nsfw_result['message']
            },
            "action": "blocked"
        }
    
    elif nsfw_result['level'] == 'mild':
        # C·∫£nh b√°o nh∆∞ng v·∫´n ch·∫°y classifier (b·ªè log)
        pass  # Mild warning, continue processing silently
    
    # B∆∞·ªõc 2: Ch·∫°y classifier (ch·ªâ khi kh√¥ng b·ªã ch·∫∑n)
    # print(f"üè∑Ô∏è  [Image {image_id}] ƒêang ph√¢n lo·∫°i ·∫£nh...")  # B·ªè log chi ti·∫øt
    x = TF(pil).unsqueeze(0).numpy()
    logits = ORT_SESS.run(["logits"], {"input": x})[0][0]
    probs = sigmoid(logits).tolist()

    # labels d√πng meta m·∫∑c ƒë·ªãnh, c√≥ th·ªÉ override
    labels = json.loads(override_labels_json) if override_labels_json else LABELS
    ranked = sorted(zip(labels, probs[:len(labels)]), key=lambda z:z[1], reverse=True)

    # C·∫£i thi·ªán logic ch·ªçn nh√£n cho ·∫£nh people
    people_score = 0
    for n, s in ranked:
        if n == "People & Events":
            people_score = s
            break
    
    # Ph√°t hi·ªán con ng∆∞·ªùi b·∫±ng computer vision
    has_people_detected = detect_people_in_image(pil)
    
    # Logic ∆∞u ti√™n People & Events:
    # 1. N·∫øu People & Events c√≥ score > 0.25 (gi·∫£m t·ª´ 0.3)
    # 2. HO·∫∂C n·∫øu ph√°t hi·ªán c√≥ ng∆∞·ªùi v√† People & Events c√≥ score > 0.15
    if people_score > 0.25 or (has_people_detected and people_score > 0.15):
        top_active = ["People & Events"]
    else:
        # Ch·ªçn c√°c nh√£n v∆∞·ª£t ng∆∞·ª°ng nh∆∞ b√¨nh th∆∞·ªùng
        top_active = [n for n,s in ranked if s >= THRESH][:3]
    
    # B∆∞·ªõc 3: T·∫°o caption b·∫±ng BLIP AI (ƒë·ªÉ l√†m context cho title generation)
    # print(f"üìù [Image {image_id}] ƒêang t·∫°o caption b·∫±ng AI...")  # B·ªè log chi ti·∫øt
    caption_en, blip_caption = generate_caption_with_blip2(pil)
    
    # N·∫øu BLIP th·∫•t b·∫°i, fallback v·ªÅ template
    if blip_caption is None:
        print(f"‚ö†Ô∏è  [Image {image_id}] Fallback v·ªÅ template caption...")
        confidence_scores = [s for n,s in ranked]
        blip_caption = generate_caption_vi(pil, top_active or [ranked[0][0]], confidence_scores)
        caption_en = None  # Kh√¥ng c√≥ caption ti·∫øng Anh
    
    # B∆∞·ªõc 4: T·∫°o ti√™u ƒë·ªÅ ti·∫øng Vi·ªát (lu√¥n lu√¥n ch·∫°y)
    caption_vi = None  # Caption ti·∫øng Vi·ªát s·∫Ω l√† title ƒë∆∞·ª£c generate
    
    if generate_title and blip_caption:
        try:
            # print(f"‚úçÔ∏è  [Image {image_id}] ƒêang t·∫°o ti√™u ƒë·ªÅ s√°ng t·∫°o...")  # B·ªè log chi ti·∫øt
            # L·∫•y category ch√≠nh (category ƒë·∫ßu ti√™n)
            main_category = top_active[0] if top_active else None
            
            # Generate titles (ch·ªâ l·∫•y 1 title ƒë·∫ßu ti√™n)
            titles_result = generate_titles(
                caption_vi=blip_caption,
                category=main_category,
                place=None,  # Auto-detect from caption
                num_titles=1  # Ch·ªâ c·∫ßn 1 title
            )
            
            if titles_result['success'] and titles_result['titles']:
                # L·∫•y title ƒë·∫ßu ti√™n l√†m caption_vi
                caption_vi = titles_result['titles'][0]
                # X√≥a d·∫•u ngo·∫∑c k√©p n·∫øu c√≥
                caption_vi = caption_vi.strip('"\'')
                # print(f"‚úÖ [Image {image_id}] ƒê√£ t·∫°o caption: {caption_vi}")  # B·ªè log chi ti·∫øt
            else:
                print(f"‚ö†Ô∏è  [Image {image_id}] Title generation failed, using BLIP caption")
                caption_vi = blip_caption
        
        except Exception as e:
            print(f"‚ö†Ô∏è  [Image {image_id}] L·ªói khi t·∫°o ti√™u ƒë·ªÅ: {e}")
            caption_vi = blip_caption
    else:
        # N·∫øu kh√¥ng generate title, d√πng BLIP caption
        caption_vi = blip_caption

    # Build response
    response = {
        "imageId": image_id,
        "nsfw_check": {
            "predicted_label": nsfw_result['predicted_label'],
            "is_violation": nsfw_result['is_violation'],
            "level": nsfw_result['level'],
            "message": nsfw_result['message']
        },
        "activeLabels": top_active,
        "caption_en": caption_en,  # BLIP caption ti·∫øng Anh g·ªëc (ƒë∆°n gi·∫£n)
        "caption_vi": caption_vi   # Title AI-generated (d√†i, chi ti·∫øt)
    }
    
    return response

@app.post("/generate-title")
async def generate_title_only(
    caption_vi: str = Form(...),
    category: Optional[str] = Form(None),
    place: Optional[str] = Form(None)
) -> Dict[str, Any]:
    """
    Generate titles from existing caption (no image analysis)
    
    Faster endpoint when you already have caption and just need titles.
    """
    try:
        result = generate_titles(
            caption_vi=caption_vi,
            category=category,
            place=place
        )
        return result
    except Exception as e:
        return {
            "titles": [],
            "place": place,
            "category": category,
            "success": False,
            "error": str(e)
        }

# (tu·ª≥ ch·ªçn) endpoint nh·∫≠n feedback song song
@app.post("/feedback")
async def feedback(payload: Dict[str, Any]):
    # B·∫°n c√≥ th·ªÉ ghi file log JSON ho·∫∑c push DB ·ªü ƒë√¢y; demo: tr·∫£ l·∫°i payload
    return {"ok": True, "received": payload}

@app.get("/health")
async def health():
    return {"status": "healthy", "service": "AI Title Service"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
